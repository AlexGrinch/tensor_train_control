{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/opensim-rl/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from utils import *\n",
    "from agent_arena_no_gathering import *\n",
    "from environments.snake.snake_env import Snake\n",
    "from environments.windy_grid_world.windy_grid_world_env import WindyGridWorld\n",
    "from environments.evil_wgw_env import EvilWindyGridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment initializtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = Snake(grid_size=(8, 8))\n",
    "#num_actions = 3\n",
    "env = EvilWindyGridWorld(grid_size=(7, 10), visual=True, stochasticity=0.01)\n",
    "num_actions = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame count: 5000\n",
      "average reward: 0.0\n",
      "epsilon: 0.975\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for stoc in [0.0, 0.02, 0.05, 0.1, 0.2]:\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "    \n",
    "        dqn_trainer = DQNTrainer(\n",
    "            num_actions, state_shape=[10, 7, 1],\n",
    "            convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64, 64],\n",
    "            gradient_clip=10.0,\n",
    "            scope=\"dqn_trainer\")\n",
    "\n",
    "        quant_trainer = QuantileTrainer(\n",
    "            num_actions, state_shape=[10, 7, 1],\n",
    "            convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "            num_atoms=64, kappa=1.0,\n",
    "            scope=\"qr_trainer\")\n",
    "        \n",
    "        cat_trainer = CategoricalTrainer(\n",
    "            num_actions, state_shape=[10, 7, 1],\n",
    "            convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "            num_atoms=64, v=(-5, 5),\n",
    "            scope=\"cat_trainer\")\n",
    "\n",
    "        trainers = [quant_trainer, dqn_trainer, cat_trainer]\n",
    "    \n",
    "        env = EvilWindyGridWorld(\n",
    "            grid_size=(7, 10), visual=True, stochasticity=stoc)\n",
    "        test_env = EvilWindyGridWorld(\n",
    "            grid_size=(7, 10), visual=True, stochasticity=stoc)\n",
    "\n",
    "        agent_arena = AgentArena(\n",
    "            env, test_env, num_actions, trainers,\n",
    "            save_path=\"evil_wgw_models/dec17/exp\"+str(stoc)[2:]+\"_\"+str(i+1))\n",
    "    \n",
    "        agent_arena.set_parameters(\n",
    "            max_episode_length=250, replay_memory_size=100000,\n",
    "            replay_start_size=250, discount_factor=0.99,\n",
    "            final_eps=0.01, annealing_steps=200000)\n",
    "    \n",
    "        agent_arena.start(\n",
    "            gpu_id=7, performance_print_freq=20, max_num_frames=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_trainer1 = DQNTrainer(\n",
    "    num_actions, state_shape=[10, 7, 1],\n",
    "    convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "    scope=\"dqn1\")\n",
    "\n",
    "dqn_trainer2 = DQNTrainer(\n",
    "    num_actions, state_shape=[10, 7, 1],\n",
    "    convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "    scope=\"dqn2\")\n",
    "\n",
    "qr_trainer = QRTrainer(\n",
    "    num_actions, state_shape=[10, 7, 1],\n",
    "    convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "    num_atoms=64, kappa=1.0,\n",
    "    scope=\"qr2\")\n",
    "\n",
    "trainers = [dqn_trainer1, dqn_trainer2, qr_trainer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EvilWindyGridWorld(grid_size=(7, 10), visual=True, stochasticity=0.05)\n",
    "test_env = EvilWindyGridWorld(grid_size=(7, 10), visual=True, stochasticity=0.05)\n",
    "\n",
    "agent_arena = AgentArena(env, test_env, num_actions, trainers, save_path=\"arena\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 10, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "agent_arena.set_parameters(\n",
    "    max_episode_length=100, replay_memory_size=100000, replay_start_size=500,\n",
    "    discount_factor=0.99, final_eps=0.01, annealing_steps=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent_arena.start(gpu_id=6, performance_print_freq=20, max_num_episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic agent which consists of two networks: agent and target.\n",
    "# Checkpoints of networks' weights and learning curves will be saved\n",
    "# in \"save_path/model_name\" folder.\n",
    "wgw_agent = DQNAgent(\n",
    "    env, num_actions, state_shape=[10, 7, 1],\n",
    "    convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "    save_path=\"evil_wgw_models\", model_name=\"dqn_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgw_agent = CatDQNAgent(\n",
    "    env, num_actions, state_shape=[10, 7, 1],\n",
    "    convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "    v=(-5, 5), num_atoms=11,\n",
    "    save_path=\"wgw_models\", model_name=\"catdqn_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgw_agent = QuantRegDQNAgent(\n",
    "    env, num_actions, state_shape=[10, 7, 1],\n",
    "    convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "    num_atoms=4,\n",
    "    optimizer=tf.train.AdamOptimizer(2.5e-4, epsilon=0.01/32),\n",
    "    save_path=\"wgw_models\", model_name=\"quantdqn4_stoc03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set basic hyper parameters (for full list see \"set_parameters\" method).\n",
    "# Create replay buffer and fill it with random transitions.\n",
    "wgw_agent.set_parameters(max_episode_length=250, replay_memory_size=100000, replay_start_size=500,\n",
    "                           discount_factor=0.99, final_eps=0.01, annealing_steps=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set training hyper parameters (for full list see \"train\" method).\n",
    "# Set gpu_id = -1 to use cpu instead if gpu, otherwise set it to gpu device id.\n",
    "wgw_agent.train(gpu_id=4, exploration=\"e-greedy\", save_freq=1000, max_num_epochs=5000, performance_print_freq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#num_atoms_range = [2, 4, 8, 16, 32, 64]\n",
    "num_atoms_range = [64] * 6\n",
    "for i, num_atoms in enumerate(num_atoms_range):\n",
    "    env = EvilWindyGridWorld(grid_size=(7, 10), visual=True, stochasticity=0.05)\n",
    "    num_actions = 4\n",
    "    \n",
    "    wgw_agent = QuantRegDQNAgent(\n",
    "        env, num_actions, state_shape=[10, 7, 1],\n",
    "        convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "        num_atoms=num_atoms,\n",
    "        optimizer=tf.train.AdamOptimizer(2.5e-4, epsilon=0.01/32),\n",
    "        save_path=\"evil_wgw_models\", model_name=\"quant64_\"+str(i+10))\n",
    "    \n",
    "    #wgw_agent = DQNAgent(\n",
    "    #    env, num_actions, state_shape=[10, 7, 1],\n",
    "    #    convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "    #    save_path=\"evil_wgw_models\", model_name=\"simple_dqn_\"+str(i+1))\n",
    "\n",
    "    # Set basic hyper parameters (for full list see \"set_parameters\" method).\n",
    "    # Create replay buffer and fill it with random transitions.\n",
    "    wgw_agent.set_parameters(\n",
    "        max_episode_length=250, replay_memory_size=100000, replay_start_size=250,\n",
    "        discount_factor=0.99, final_eps=0.01, annealing_steps=100000)\n",
    "\n",
    "    # Set training hyper parameters (for full list see \"train\" method).\n",
    "    # Set gpu_id = -1 to use cpu instead if gpu, otherwise set it to gpu device id.\n",
    "    wgw_agent.train(\n",
    "        gpu_id=0, exploration=\"e-greedy\", save_freq=1000,\n",
    "        max_num_epochs=1500, performance_print_freq=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic deep Q-network\n",
    "snake_agent = DQNAgent(env, num_actions, state_shape=[8, 8, 5],\n",
    "                       convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[128],\n",
    "                       save_path=\"snake_models\", model_name=\"dqn_8x8\")\n",
    "\n",
    "# Dueling deep Q-network\n",
    "snake_agent = DuelDQNAgent(env, num_actions, state_shape=[8, 8, 5],\n",
    "                           convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[64],\n",
    "                           save_path=\"snake_models\", model_name=\"dueldqn_8x8\")\n",
    "\n",
    "# Categorical deep Q-network (C51)\n",
    "snake_agent = CatDQNAgent(env, num_actions, state_shape=[8, 8, 5],\n",
    "                          convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[128],\n",
    "                          v=(-5, 25), num_atoms=51,\n",
    "                          save_path=\"snake_models\", model_name=\"catdqn_8x8\")\n",
    "\n",
    "# Quantile regression deep Q-network (QR-DQN)\n",
    "snake_agent = QuantRegDQNAgent(env, num_actions, state_shape=[8, 8, 5],\n",
    "                               convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[128],\n",
    "                               num_atoms=100, kappa=1.0,\n",
    "                               save_path=\"snake_models\", model_name=\"quantdqn_8x8\")\n",
    "\n",
    "# Soft Actor-Critic\n",
    "snake_agent = SACAgent(env, num_actions, state_shape=[8, 8, 5],\n",
    "                       convs=[[16, 2, 1], [32, 1, 1]], fully_connected=[128],\n",
    "                       temperature=0.1,\n",
    "                       save_path=\"snake_models\", model_name=\"sac_8x8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Atari Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment initializtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = \"PongNoFrameskip-v4\"\n",
    "env = wrap_deepmind(gym.make(game_id))\n",
    "num_actions = env.unwrapped.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari_agent = DQNAgent(env, num_actions, state_shape=[84, 84, 4],\n",
    "                       convs=[[32, 8, 4], [64, 4, 2], [64, 3, 1]], fully_connected=[512],\n",
    "                       save_path=\"atari_models\", model_name=\"dqn_boi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari_agent.set_parameters(max_episode_length=100000, discount_factor=0.99, final_eps=0.01,\n",
    "                           replay_memory_size=1000000, replay_start_size=50, annealing_steps=1000000,\n",
    "                           frame_history_len=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari_agent.train(gpu_id=-1, exploration=\"e-greedy\", save_freq=50000, \n",
    "                  max_num_epochs=1000, performance_print_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic deep Q-network\n",
    "atari_agent = DQNAgent(env, num_actions, state_shape=[84, 84, 4],\n",
    "                       convs=[[32, 8, 4], [64, 4, 2], [64, 3, 1]], fully_connected=[512],\n",
    "                       save_path=\"atari_models\", model_name=\"dqn_boi\")\n",
    "\n",
    "# Dueling deep Q-network\n",
    "atari_agent = DuelDQNAgent(env, num_actions, state_shape=[84, 84, 4],\n",
    "                           convs=[[32, 8, 4], [64, 4, 2], [64, 3, 1]], fully_connected=[256],\n",
    "                           save_path=\"atari_models\", model_name=\"dueldqn_boi\")\n",
    "\n",
    "# Categorical deep Q-network (C51)\n",
    "atari_agent = CatDQNAgent(env, num_actions, state_shape=[84, 84, 4],\n",
    "                          convs=[[32, 8, 4], [64, 4, 2], [64, 3, 1]], fully_connected=[512],\n",
    "                          v=(-10, 10), num_atoms=51,\n",
    "                          save_path=\"atari_models\", model_name=\"catdqn_boi\")\n",
    "\n",
    "# Quantile regression deep Q-network (QR-DQN)\n",
    "atari_agent = QuantRegDQNAgent(env, num_actions, state_shape=[84, 84, 4],\n",
    "                               convs=[[32, 8, 4], [64, 4, 2], [64, 3, 1]], fully_connected=[512],\n",
    "                               num_atoms=200, kappa=1,\n",
    "                               save_path=\"atari_models\", model_name=\"quantdqn_boi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
